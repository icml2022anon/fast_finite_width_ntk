{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otnqBdyaYBrf"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/icml2022anon/fast_finite_width_ntk/blob/main/example_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTt0UNQbk_Td"
      },
      "source": [
        "# Example of computing NTK of a **PyTorch** FCN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvXMUSdFjCqq"
      },
      "source": [
        "Tested on NVIDIA A100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZGtR-ES1ZoP"
      },
      "source": [
        "More examples: \n",
        "\n",
        "\n",
        "*   [JAX (Flax)](https://colab.research.google.com/github/icml2022anon/fast_finite_width_ntk/blob/main/example.ipynb)\n",
        "*   [TensorFlow (Keras)](https://colab.research.google.com/github/icml2022anon/fast_finite_width_ntk/blob/main/example_tf.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl2JyRE1hK-z"
      },
      "source": [
        "# Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx1xr9v5pyl0",
        "outputId": "eff9e0dc-f73b-4dde-f79b-2558c4fefd6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: A100-SXM4-40GB (UUID: GPU-f3b01e18-1bac-8bc5-09d7-b3075ae5c01d)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlckZChsxTWj",
        "outputId": "65de0e08-e55a-4be8-e1ff-d94270665174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (22.0.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://storage.googleapis.com/jax-releases/jax_releases.html\n",
            "Requirement already satisfied: jax[cuda11_cudnn805] in /usr/local/lib/python3.7/dist-packages (0.3.5)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn805]) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn805]) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn805]) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn805]) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn805]) (3.3.0)\n",
            "Requirement already satisfied: jaxlib==0.3.5+cuda11.cudnn805 in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn805]) (0.3.5+cuda11.cudnn805)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.3.5+cuda11.cudnn805->jax[cuda11_cudnn805]) (2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax[cuda11_cudnn805]) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# We need at least jaxlib-0.1.73 to avoid certain CUDA bugs when using `implementation=auto`\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade jax[cuda11_cudnn805] -f https://storage.googleapis.com/jax-releases/jax_releases.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db5zKGndS1zH",
        "outputId": "211cd1a7-94b1-4c4e-f1e4-0c91f7012ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/deepmind/tf2jax.git\n",
            "  Cloning https://github.com/deepmind/tf2jax.git to /tmp/pip-req-build-vn3jkvr9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepmind/tf2jax.git /tmp/pip-req-build-vn3jkvr9\n",
            "  Resolved https://github.com/deepmind/tf2jax.git to commit b3e80f7e9ac18d7d495b995b2e63c182aee6b236\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/icml2022anon/fast_finite_width_ntk.git\n",
            "  Cloning https://github.com/icml2022anon/fast_finite_width_ntk.git to /tmp/pip-req-build-1pq9dhf3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/icml2022anon/fast_finite_width_ntk.git /tmp/pip-req-build-1pq9dhf3\n",
            "  Resolved https://github.com/icml2022anon/fast_finite_width_ntk.git to commit 23b5988ed95f58c55c9a9b7cb7e707b9e0f018c1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jax>=0.2.25 in /usr/local/lib/python3.7/dist-packages (from fast-finite-width-ntk==0.0.2) (0.3.5)\n",
            "Requirement already satisfied: tensorflow==2.7.0 in /usr/local/lib/python3.7/dist-packages (from fast-finite-width-ntk==0.0.2) (2.7.0)\n",
            "Requirement already satisfied: torch==1.10 in /usr/local/lib/python3.7/dist-packages (from fast-finite-width-ntk==0.0.2) (1.10.0+cu111)\n",
            "Requirement already satisfied: onnx2keras==0.0.24 in /usr/local/lib/python3.7/dist-packages (from fast-finite-width-ntk==0.0.2) (0.0.24)\n",
            "Requirement already satisfied: onnx==1.11.0 in /usr/local/lib/python3.7/dist-packages (from fast-finite-width-ntk==0.0.2) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx==1.11.0->fast-finite-width-ntk==0.0.2) (3.10.0.2)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx==1.11.0->fast-finite-width-ntk==0.0.2) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx==1.11.0->fast-finite-width-ntk==0.0.2) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.13.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (0.23.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (12.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.6.3)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (0.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.43.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.25->fast-finite-width-ntk==0.0.2) (1.4.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (3.3.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (4.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->fast-finite-width-ntk==0.0.2) (3.1.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/deepmind/tf2jax.git --no-deps\n",
        "!pip install git+https://github.com/icml2022anon/fast_finite_width_ntk.git --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LbW8KVnsPfVd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from fast_finite_width_ntk import empirical_ntk_fn_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VWfaferhCpIk"
      },
      "outputs": [],
      "source": [
        "input_size = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpIgfQ1dCZcr"
      },
      "source": [
        "# PyTorch model defintion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "doEDxcVFCZIp"
      },
      "outputs": [],
      "source": [
        "def get_model(O: int):\n",
        "  # TODO: match ONNX and Pytorch tree structures and tensor layouts for CNNs.\n",
        "  return torch.nn.Sequential(\n",
        "      torch.nn.Linear(input_size, 2048),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(2048, 2048),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(2048, 2048),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(2048, O),\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXqlToEouqSc"
      },
      "source": [
        "# NTK functions declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lPh5LGz9JBK_"
      },
      "outputs": [],
      "source": [
        "def get_ntk_fns(O: int):\n",
        "  # Define a PyTorch with `O` output logits.\n",
        "  f = get_model(O)\n",
        "  f.forward(torch.rand((1, input_size)))\n",
        "  params = [p.T if p.ndim == 2 else p for p in f.parameters()]\n",
        "\n",
        "  kwargs = dict(\n",
        "      f=f,\n",
        "      input_shape=(input_size,),\n",
        "      trace_axes=(),\n",
        "      vmap_axes=0\n",
        "  )\n",
        "\n",
        "  # Different NTK implementations\n",
        "  jacobian_contraction = empirical_ntk_fn_pytorch(**kwargs, implementation=1)\n",
        "  ntvp = empirical_ntk_fn_pytorch(**kwargs, implementation=2)\n",
        "  str_derivatives = empirical_ntk_fn_pytorch(**kwargs, implementation=3)\n",
        "  auto = empirical_ntk_fn_pytorch(**kwargs, implementation=0)\n",
        "  \n",
        "  return params, (jacobian_contraction, ntvp, str_derivatives, auto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh_aFUZFXm_C"
      },
      "source": [
        "# $\\color{blue}O = 10$ logits, batch size $\\color{red}N = 10$, NTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rs4KF07fjjv"
      },
      "source": [
        "Structured derivatives allows to compute the NTK much faster than other methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z4MpdSJ7hFsD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb94a0ee-89ea-477b-bed0-b9b2aba6167d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fast_finite_width_ntk/empirical_pytorch.py:188: UserWarning: This function is an early proof-of-concept.\n",
            "  warnings.warn('This function is an early proof-of-concept.')\n",
            "INFO:onnx2keras:Converter is called.\n",
            "DEBUG:onnx2keras:List input shapes:\n",
            "DEBUG:onnx2keras:None\n",
            "DEBUG:onnx2keras:List inputs:\n",
            "DEBUG:onnx2keras:Input 0 -> input.\n",
            "DEBUG:onnx2keras:List outputs:\n",
            "DEBUG:onnx2keras:Output 0 -> output.\n",
            "DEBUG:onnx2keras:Gathering weights to dictionary.\n",
            "DEBUG:onnx2keras:Found weight 0.weight with shape (2048, 1024).\n",
            "DEBUG:onnx2keras:Found weight 0.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 2.weight with shape (2048, 2048).\n",
            "DEBUG:onnx2keras:Found weight 2.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 4.weight with shape (2048, 2048).\n",
            "DEBUG:onnx2keras:Found weight 4.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 6.weight with shape (10, 2048).\n",
            "DEBUG:onnx2keras:Found weight 6.bias with shape (10,).\n",
            "DEBUG:onnx2keras:Found input input with shape [1024]\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 9\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name input).\n",
            "DEBUG:onnx2keras:Check input 1 (name 0.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 0.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 1024, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='9/BiasAdd:0', description=\"created by layer '9'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 10\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 9).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='10/Relu:0', description=\"created by layer '10'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 11\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 10).\n",
            "DEBUG:onnx2keras:Check input 1 (name 2.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 2.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='11/BiasAdd:0', description=\"created by layer '11'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 12\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 11).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='12/Relu:0', description=\"created by layer '12'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 13\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 12).\n",
            "DEBUG:onnx2keras:Check input 1 (name 4.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 4.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='13/BiasAdd:0', description=\"created by layer '13'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 14\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 13).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='14/Relu:0', description=\"created by layer '14'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: output\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 14).\n",
            "DEBUG:onnx2keras:Check input 1 (name 6.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 6.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 10.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='output/BiasAdd:0', description=\"created by layer 'output'\")\n",
            "INFO:onnx2keras:Converter is called.\n",
            "DEBUG:onnx2keras:List input shapes:\n",
            "DEBUG:onnx2keras:None\n",
            "DEBUG:onnx2keras:List inputs:\n",
            "DEBUG:onnx2keras:Input 0 -> input.\n",
            "DEBUG:onnx2keras:List outputs:\n",
            "DEBUG:onnx2keras:Output 0 -> output.\n",
            "DEBUG:onnx2keras:Gathering weights to dictionary.\n",
            "DEBUG:onnx2keras:Found weight 0.weight with shape (2048, 1024).\n",
            "DEBUG:onnx2keras:Found weight 0.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 2.weight with shape (2048, 2048).\n",
            "DEBUG:onnx2keras:Found weight 2.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 4.weight with shape (2048, 2048).\n",
            "DEBUG:onnx2keras:Found weight 4.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 6.weight with shape (10, 2048).\n",
            "DEBUG:onnx2keras:Found weight 6.bias with shape (10,).\n",
            "DEBUG:onnx2keras:Found input input with shape [1024]\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 9\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name input).\n",
            "DEBUG:onnx2keras:Check input 1 (name 0.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 0.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 1024, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='9/BiasAdd:0', description=\"created by layer '9'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 10\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 9).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='10/Relu:0', description=\"created by layer '10'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 11\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 10).\n",
            "DEBUG:onnx2keras:Check input 1 (name 2.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 2.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='11/BiasAdd:0', description=\"created by layer '11'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 12\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 11).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='12/Relu:0', description=\"created by layer '12'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 13\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 12).\n",
            "DEBUG:onnx2keras:Check input 1 (name 4.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 4.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='13/BiasAdd:0', description=\"created by layer '13'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 14\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 13).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='14/Relu:0', description=\"created by layer '14'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: output\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 14).\n",
            "DEBUG:onnx2keras:Check input 1 (name 6.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 6.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 10.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='output/BiasAdd:0', description=\"created by layer 'output'\")\n",
            "INFO:onnx2keras:Converter is called.\n",
            "DEBUG:onnx2keras:List input shapes:\n",
            "DEBUG:onnx2keras:None\n",
            "DEBUG:onnx2keras:List inputs:\n",
            "DEBUG:onnx2keras:Input 0 -> input.\n",
            "DEBUG:onnx2keras:List outputs:\n",
            "DEBUG:onnx2keras:Output 0 -> output.\n",
            "DEBUG:onnx2keras:Gathering weights to dictionary.\n",
            "DEBUG:onnx2keras:Found weight 0.weight with shape (2048, 1024).\n",
            "DEBUG:onnx2keras:Found weight 0.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 2.weight with shape (2048, 2048).\n",
            "DEBUG:onnx2keras:Found weight 2.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 4.weight with shape (2048, 2048).\n",
            "DEBUG:onnx2keras:Found weight 4.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 6.weight with shape (10, 2048).\n",
            "DEBUG:onnx2keras:Found weight 6.bias with shape (10,).\n",
            "DEBUG:onnx2keras:Found input input with shape [1024]\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 9\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name input).\n",
            "DEBUG:onnx2keras:Check input 1 (name 0.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 0.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 1024, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='9/BiasAdd:0', description=\"created by layer '9'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 10\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 9).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='10/Relu:0', description=\"created by layer '10'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 11\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 10).\n",
            "DEBUG:onnx2keras:Check input 1 (name 2.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 2.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='11/BiasAdd:0', description=\"created by layer '11'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 12\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 11).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='12/Relu:0', description=\"created by layer '12'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 13\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 12).\n",
            "DEBUG:onnx2keras:Check input 1 (name 4.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 4.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='13/BiasAdd:0', description=\"created by layer '13'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 14\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 13).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='14/Relu:0', description=\"created by layer '14'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: output\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 14).\n",
            "DEBUG:onnx2keras:Check input 1 (name 6.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 6.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 10.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='output/BiasAdd:0', description=\"created by layer 'output'\")\n",
            "INFO:onnx2keras:Converter is called.\n",
            "DEBUG:onnx2keras:List input shapes:\n",
            "DEBUG:onnx2keras:None\n",
            "DEBUG:onnx2keras:List inputs:\n",
            "DEBUG:onnx2keras:Input 0 -> input.\n",
            "DEBUG:onnx2keras:List outputs:\n",
            "DEBUG:onnx2keras:Output 0 -> output.\n",
            "DEBUG:onnx2keras:Gathering weights to dictionary.\n",
            "DEBUG:onnx2keras:Found weight 0.weight with shape (2048, 1024).\n",
            "DEBUG:onnx2keras:Found weight 0.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 2.weight with shape (2048, 2048).\n",
            "DEBUG:onnx2keras:Found weight 2.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 4.weight with shape (2048, 2048).\n",
            "DEBUG:onnx2keras:Found weight 4.bias with shape (2048,).\n",
            "DEBUG:onnx2keras:Found weight 6.weight with shape (10, 2048).\n",
            "DEBUG:onnx2keras:Found weight 6.bias with shape (10,).\n",
            "DEBUG:onnx2keras:Found input input with shape [1024]\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 9\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name input).\n",
            "DEBUG:onnx2keras:Check input 1 (name 0.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 0.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 1024, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='9/BiasAdd:0', description=\"created by layer '9'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 10\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 9).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='10/Relu:0', description=\"created by layer '10'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 11\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 10).\n",
            "DEBUG:onnx2keras:Check input 1 (name 2.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 2.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='11/BiasAdd:0', description=\"created by layer '11'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 12\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 11).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='12/Relu:0', description=\"created by layer '12'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: 13\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 12).\n",
            "DEBUG:onnx2keras:Check input 1 (name 4.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 4.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 2048.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='13/BiasAdd:0', description=\"created by layer '13'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Relu\n",
            "DEBUG:onnx2keras:node_name: 14\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 13).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='14/Relu:0', description=\"created by layer '14'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Gemm\n",
            "DEBUG:onnx2keras:node_name: output\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 14).\n",
            "DEBUG:onnx2keras:Check input 1 (name 6.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 2 (name 6.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
            "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
            "DEBUG:onnx2keras:gemm:Input units 2048, output units 10.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='output/BiasAdd:0', description=\"created by layer 'output'\")\n"
          ]
        }
      ],
      "source": [
        "O = 10\n",
        "N = 10\n",
        "\n",
        "# Input images x\n",
        "x1 = torch.rand((N, input_size))\n",
        "x2 = torch.rand((N, input_size))\n",
        "\n",
        "params, (ntk_fn_jacobian_contraction, ntk_fn_ntvp, ntk_fn_str_derivatives, ntk_fn_auto) = get_ntk_fns(O=O)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zObT8WnPggFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4732729a-4c98-40d0-c8c6-a8e8edd6e5c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:absl:Initializing backend 'interpreter'\n",
            "DEBUG:absl:Backend 'interpreter' initialized\n",
            "DEBUG:absl:Initializing backend 'cpu'\n",
            "DEBUG:absl:Backend 'cpu' initialized\n",
            "DEBUG:absl:Initializing backend 'tpu_driver'\n",
            "INFO:absl:Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n",
            "DEBUG:absl:Initializing backend 'gpu'\n",
            "DEBUG:absl:Backend 'gpu' initialized\n",
            "DEBUG:absl:Initializing backend 'tpu'\n",
            "INFO:absl:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005383491516113281 sec\n",
            "DEBUG:absl:Compiling prim_fun (140285108449760 for args (ShapedArray(float32[10,1024]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.19342517852783203 sec\n",
            "DEBUG:absl:Finished tracing + transforming matmul for jit in 0.0016980171203613281 sec\n",
            "DEBUG:absl:Compiling matmul (140287053802864 for args (ShapedArray(float32[10,1,1024]), ShapedArray(float32[1024,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(matmul)) in 0.010773658752441406 sec\n",
            "DEBUG:absl:Finished tracing + transforming fn for jit in 0.0023522377014160156 sec\n",
            "DEBUG:absl:Compiling fn (140287053801344 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(fn)) in 0.015099763870239258 sec\n",
            "DEBUG:absl:Finished tracing + transforming relu for jit in 0.0016143321990966797 sec\n",
            "DEBUG:absl:Compiling relu (140287054618528 for args (ShapedArray(float32[10,1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(relu) in 0.01503896713256836 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00028061866760253906 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00041413307189941406 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287290491440 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[])).\n",
            "DEBUG:absl:Finished XLA compilation of gt in 0.016832828521728516 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0004169940948486328 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287053871056 for args (ShapedArray(float32[]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.5089960098266602 sec\n",
            "DEBUG:absl:Finished tracing + transforming matmul for jit in 0.002057790756225586 sec\n",
            "DEBUG:absl:Compiling matmul (140287290277072 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[2048,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(matmul)) in 0.010102510452270508 sec\n",
            "DEBUG:absl:Finished tracing + transforming matmul for jit in 0.0019197463989257812 sec\n",
            "DEBUG:absl:Compiling matmul (140287055021632 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[2048,10])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(matmul)) in 0.010556221008300781 sec\n",
            "DEBUG:absl:Finished tracing + transforming fn for jit in 0.002065420150756836 sec\n",
            "DEBUG:absl:Compiling fn (140287053955792 for args (ShapedArray(float32[10,1,10]), ShapedArray(float32[10])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(fn)) in 0.01336050033569336 sec\n",
            "DEBUG:absl:Finished tracing + transforming _squeeze for jit in 0.0006930828094482422 sec\n",
            "DEBUG:absl:Compiling _squeeze (140287054224352 for args (ShapedArray(float32[10,1,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(_squeeze)) in 0.007887601852416992 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0002665519714355469 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287054049600 for args ().\n",
            "DEBUG:absl:Finished XLA compilation of iota in 0.06711935997009277 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0006372928619384766 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287054049600 for args (ShapedArray(int32[10,10]), ShapedArray(int32[])).\n",
            "DEBUG:absl:Finished XLA compilation of add in 0.06833314895629883 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.000583648681640625 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287054034816 for args ().\n",
            "DEBUG:absl:Finished XLA compilation of iota in 0.06748533248901367 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0007212162017822266 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287053801744 for args (ShapedArray(int32[10,10]), ShapedArray(int32[10,10])).\n",
            "DEBUG:absl:Finished XLA compilation of eq in 0.06961822509765625 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005240440368652344 sec\n",
            "DEBUG:absl:Compiling prim_fun (140285108477376 for args (ShapedArray(bool[10,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of convert_element_type in 0.06667017936706543 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0007863044738769531 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287053975456 for args (ShapedArray(float32[10,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of slice in 0.0030846595764160156 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0007200241088867188 sec\n",
            "DEBUG:absl:Compiling backward_pass (140287054180832 for args (ShapedArray(float32[10,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(transpose(jvp(_squeeze))) in 0.0035407543182373047 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0015234947204589844 sec\n",
            "DEBUG:absl:Compiling backward_pass (140287054182832 for args (ShapedArray(float32[10,1,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(transpose(jvp(fn))) in 0.0045316219329833984 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.001821756362915039 sec\n",
            "DEBUG:absl:Compiling backward_pass (140287053925744 for args (ShapedArray(float32[2048,10]), ShapedArray(float32[10,1,2048]), ShapedArray(float32[10,1,10])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(transpose(jvp(matmul)))) in 0.03581643104553223 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0003418922424316406 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287054181712 for args (ShapedArray(bool[10,1,2048]), ShapedArray(bool[])).\n",
            "DEBUG:absl:Finished XLA compilation of eq in 0.014484882354736328 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0006275177001953125 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287054166368 for args (ShapedArray(bool[10,1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.02813100814819336 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005865097045898438 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287054418880 for args (ShapedArray(float32[1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.06910896301269531 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0006492137908935547 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287054076096 for args (ShapedArray(float32[10,1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.0704345703125 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0006616115570068359 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287054181552 for args (ShapedArray(float32[10,1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.020912885665893555 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0003840923309326172 sec\n",
            "DEBUG:absl:Compiling prim_fun (140285108266320 for args (ShapedArray(bool[10,10,1,2048]), ShapedArray(float32[10,10,1,2048]), ShapedArray(float32[10,10,1,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of select_n in 0.02031564712524414 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0019309520721435547 sec\n",
            "DEBUG:absl:Compiling backward_pass (140287054167328 for args (ShapedArray(float32[10,10,1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(transpose(jvp(fn)))) in 0.00935983657836914 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.002246856689453125 sec\n",
            "DEBUG:absl:Compiling backward_pass (140287054180752 for args (ShapedArray(float32[2048,2048]), ShapedArray(float32[10,1,2048]), ShapedArray(float32[10,10,1,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(transpose(jvp(matmul)))) in 0.03182411193847656 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0011980533599853516 sec\n",
            "DEBUG:absl:Compiling backward_pass (140287053756448 for args (ShapedArray(float32[10,1,1024]), ShapedArray(float32[10,10,1,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(transpose(jvp(matmul)))) in 0.021597862243652344 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005135536193847656 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287053882624 for args (ShapedArray(float32[10,10,1024,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of slice in 0.008446216583251953 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005025863647460938 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338579808 for args (ShapedArray(float32[10,10,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of slice in 0.008700132369995117 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.000457763671875 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338574512 for args (ShapedArray(float32[10,10,2048,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of slice in 0.007809638977050781 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0006031990051269531 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338639728 for args (ShapedArray(float32[10,10,2048,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of slice in 0.010897159576416016 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00046634674072265625 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287054181232 for args (ShapedArray(float32[10,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.07194828987121582 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00057220458984375 sec\n",
            "DEBUG:absl:Compiling prim_fun (140294019505360 for args (ShapedArray(float32[10,10,1024,2048]), ShapedArray(float32[10,10,1024,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of dot_general in 0.022410154342651367 sec\n",
            "DEBUG:absl:Finished tracing + transforming _moveaxis for jit in 0.00054168701171875 sec\n",
            "DEBUG:absl:Compiling _moveaxis (140284338594592 for args (ShapedArray(float32[10,10,10,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of _moveaxis in 0.021390438079833984 sec\n",
            "DEBUG:absl:Finished tracing + transforming _moveaxis for jit in 0.00048232078552246094 sec\n",
            "DEBUG:absl:Finished tracing + transforming true_divide for jit in 0.0006208419799804688 sec\n",
            "DEBUG:absl:Compiling true_divide (140285108297968 for args (ShapedArray(float32[10,10,10,10]), ShapedArray(int32[], weak_type=True)).\n",
            "DEBUG:absl:Finished XLA compilation of true_divide in 0.020756244659423828 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005986690521240234 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287053909360 for args (ShapedArray(float32[10,10,2048]), ShapedArray(float32[10,10,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of dot_general in 0.01855635643005371 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0004897117614746094 sec\n",
            "DEBUG:absl:Compiling prim_fun (140285108297968 for args (ShapedArray(float32[10,10,2048,2048]), ShapedArray(float32[10,10,2048,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of dot_general in 0.020261526107788086 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005908012390136719 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338677072 for args (ShapedArray(float32[10,10,2048,10]), ShapedArray(float32[10,10,2048,10])).\n",
            "DEBUG:absl:Finished XLA compilation of dot_general in 0.024321556091308594 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005526542663574219 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287053929360 for args (ShapedArray(float32[10,10,10]), ShapedArray(float32[10,10,10])).\n",
            "DEBUG:absl:Finished XLA compilation of dot_general in 1.1294338703155518 sec\n",
            "DEBUG:absl:Finished tracing + transforming _moveaxis for jit in 0.0007033348083496094 sec\n",
            "DEBUG:absl:Compiling _moveaxis (140284338728464 for args (ShapedArray(float32[10,10,10,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of _moveaxis in 0.07508325576782227 sec\n",
            "DEBUG:absl:Finished tracing + transforming _moveaxis for jit in 0.0003135204315185547 sec\n",
            "DEBUG:absl:Finished tracing + transforming true_divide for jit in 0.0006206035614013672 sec\n",
            "DEBUG:absl:Compiling true_divide (140284338715776 for args (ShapedArray(float32[10,10,10,10]), ShapedArray(int32[], weak_type=True)).\n",
            "DEBUG:absl:Finished XLA compilation of true_divide in 0.07025027275085449 sec\n",
            "DEBUG:absl:Finished tracing + transforming fn for jit in 0.0006604194641113281 sec\n",
            "DEBUG:absl:Compiling fn (140284338611696 for args (ShapedArray(float32[10,10,10,10]), ShapedArray(float32[10,10,10,10])).\n",
            "DEBUG:absl:Finished XLA compilation of fn in 0.025727510452270508 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 10, 10, 10])\n"
          ]
        }
      ],
      "source": [
        "# Jacobian contraction\n",
        "k_1 = ntk_fn_jacobian_contraction(x1, x2, params)\n",
        "print(k_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FW9gJJ4qggFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02d0c8e-ce3e-4ddc-9f31-049f4bfd4735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:absl:Finished tracing + transforming matmul for jit in 0.0014281272888183594 sec\n",
            "DEBUG:absl:Compiling matmul (140284338827904 for args (ShapedArray(float32[10,1,1024]), ShapedArray(float32[1024,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(matmul)) in 0.010274410247802734 sec\n",
            "DEBUG:absl:Finished tracing + transforming fn for jit in 0.0020987987518310547 sec\n",
            "DEBUG:absl:Compiling fn (140284338321760 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(fn)) in 0.015392303466796875 sec\n",
            "DEBUG:absl:Finished tracing + transforming matmul for jit in 0.0018317699432373047 sec\n",
            "DEBUG:absl:Compiling matmul (140284338807424 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[2048,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(matmul)) in 0.010088682174682617 sec\n",
            "DEBUG:absl:Finished tracing + transforming matmul for jit in 0.0031430721282958984 sec\n",
            "DEBUG:absl:Compiling matmul (140284338808624 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[2048,10])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(matmul)) in 0.010149478912353516 sec\n",
            "DEBUG:absl:Finished tracing + transforming fn for jit in 0.0017802715301513672 sec\n",
            "DEBUG:absl:Compiling fn (140284338395088 for args (ShapedArray(float32[10,1,10]), ShapedArray(float32[10])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(fn)) in 0.013405084609985352 sec\n",
            "DEBUG:absl:Finished tracing + transforming _squeeze for jit in 0.0008873939514160156 sec\n",
            "DEBUG:absl:Compiling _squeeze (140284338395088 for args (ShapedArray(float32[10,1,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(_squeeze)) in 0.007498264312744141 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0005600452423095703 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0010004043579101562 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0015590190887451172 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0007269382476806641 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0012671947479248047 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0008258819580078125 sec\n",
            "DEBUG:absl:Finished tracing + transforming matmul for jit in 0.0015897750854492188 sec\n",
            "DEBUG:absl:Compiling matmul (140284338381600 for args (ShapedArray(float32[10,1,1024]), ShapedArray(float32[1024,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(matmul)) in 0.009066104888916016 sec\n",
            "DEBUG:absl:Finished tracing + transforming fn for jit in 0.002127408981323242 sec\n",
            "DEBUG:absl:Compiling fn (140284338804368 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(fn)) in 0.014742612838745117 sec\n",
            "DEBUG:absl:Finished tracing + transforming relu for jit in 0.0014033317565917969 sec\n",
            "DEBUG:absl:Compiling relu (140284338527360 for args (ShapedArray(float32[10,1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(relu) in 0.01311945915222168 sec\n",
            "DEBUG:absl:Finished tracing + transforming matmul for jit in 0.002115488052368164 sec\n",
            "DEBUG:absl:Compiling matmul (140284338540208 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[2048,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(matmul)) in 0.008952140808105469 sec\n",
            "DEBUG:absl:Finished tracing + transforming matmul for jit in 0.0017209053039550781 sec\n",
            "DEBUG:absl:Compiling matmul (140284338061392 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[2048,10])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(matmul)) in 0.010056734085083008 sec\n",
            "DEBUG:absl:Finished tracing + transforming fn for jit in 0.0019087791442871094 sec\n",
            "DEBUG:absl:Compiling fn (140284338075120 for args (ShapedArray(float32[10,1,10]), ShapedArray(float32[10])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(fn)) in 0.013371706008911133 sec\n",
            "DEBUG:absl:Finished tracing + transforming _squeeze for jit in 0.0008943080902099609 sec\n",
            "DEBUG:absl:Compiling _squeeze (140284338088288 for args (ShapedArray(float32[10,1,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(jvp(_squeeze)) in 0.007837295532226562 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0006988048553466797 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338055520 for args (ShapedArray(float32[10,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(transpose(jvp(_squeeze))) in 0.003205537796020508 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0012240409851074219 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338828464 for args (ShapedArray(float32[10,1,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(transpose(jvp(fn))) in 0.0036292076110839844 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0017731189727783203 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338375648 for args (ShapedArray(float32[2048,10]), ShapedArray(float32[10,1,2048]), ShapedArray(float32[10,1,10])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(transpose(jvp(matmul)))) in 0.032703399658203125 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0013492107391357422 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338406576 for args (ShapedArray(float32[10,10,1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(transpose(jvp(fn)))) in 0.008702993392944336 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0022118091583251953 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338697792 for args (ShapedArray(float32[2048,2048]), ShapedArray(float32[10,1,2048]), ShapedArray(float32[10,10,1,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(transpose(jvp(matmul)))) in 0.030951738357543945 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0014581680297851562 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338189168 for args (ShapedArray(float32[10,1,1024]), ShapedArray(float32[10,10,1,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(transpose(jvp(matmul)))) in 0.02116703987121582 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0017826557159423828 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338192048 for args (ShapedArray(float32[10,1,1024]), ShapedArray(float32[10,10,1024,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(vmap(transpose(transpose(jvp(matmul)))))) in 0.03610110282897949 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.002599000930786133 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338408336 for args (ShapedArray(float32[10,10,10,1,2048]), ShapedArray(float32[10,10,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(vmap(transpose(transpose(jvp(fn)))))) in 0.02142620086669922 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0003972053527832031 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338102592 for args (ShapedArray(bool[10,10,1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.04895949363708496 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0006105899810791016 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338175824 for args (ShapedArray(float32[10,10,1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.07202911376953125 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0003979206085205078 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338074800 for args (ShapedArray(bool[10,10,10,1,2048]), ShapedArray(float32[10,10,10,1,2048]), ShapedArray(float32[10,10,10,1,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of select_n in 0.021880388259887695 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0029268264770507812 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338074800 for args (ShapedArray(float32[2048,2048]), ShapedArray(float32[10,1,2048]), ShapedArray(float32[10,10,10,1,2048]), ShapedArray(float32[10,10,2048,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(vmap(transpose(transpose(jvp(matmul)))))) in 0.03864598274230957 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.002796173095703125 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338116480 for args (ShapedArray(float32[2048,10]), ShapedArray(float32[10,1,2048]), ShapedArray(float32[10,10,10,1,2048]), ShapedArray(float32[10,10,2048,10])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(vmap(transpose(transpose(jvp(matmul)))))) in 0.05193209648132324 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0022029876708984375 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338207152 for args (ShapedArray(float32[10,10,10,1,10]), ShapedArray(float32[10,10])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(vmap(transpose(transpose(jvp(fn)))))) in 0.02389383316040039 sec\n",
            "DEBUG:absl:Finished tracing + transforming backward_pass for jit in 0.0018405914306640625 sec\n",
            "DEBUG:absl:Compiling backward_pass (140284338106272 for args (ShapedArray(float32[10,10,10,1,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(vmap(vmap(transpose(transpose(jvp(_squeeze)))))) in 0.007700681686401367 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00044846534729003906 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338368752 for args (ShapedArray(float32[10,10,10,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of slice in 0.007186412811279297 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00034737586975097656 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287054107424 for args (ShapedArray(float32[10,10,10,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of transpose in 0.019643068313598633 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 10, 10, 10])\n"
          ]
        }
      ],
      "source": [
        "# NTK-vector products\n",
        "k_2 = ntk_fn_ntvp(x1, x2, params)\n",
        "print(k_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gFeWnqGQggFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed4b472-a714-4e9a-f44b-263fe8a3dfd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00038623809814453125 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338062192 for args (ShapedArray(float32[10,1,1024]), ShapedArray(float32[1024,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of dot_general in 0.016028881072998047 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0003523826599121094 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338779152 for args (ShapedArray(float32[2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.007985830307006836 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005848407745361328 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338373488 for args (ShapedArray(float32[1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.00751805305480957 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005481243133544922 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338486320 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[1,1,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of add in 0.014130353927612305 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005388259887695312 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338539168 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[])).\n",
            "DEBUG:absl:Finished XLA compilation of max in 0.012640953063964844 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0004520416259765625 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338447776 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[2048,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of dot_general in 0.01563405990600586 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005958080291748047 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337797472 for args (ShapedArray(float32[10,1,2048]), ShapedArray(float32[2048,10])).\n",
            "DEBUG:absl:Finished XLA compilation of dot_general in 0.013677120208740234 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00036072731018066406 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337809760 for args (ShapedArray(float32[10]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.0074460506439208984 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00038886070251464844 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338309392 for args (ShapedArray(float32[1,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.0070650577545166016 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0003314018249511719 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338307232 for args (ShapedArray(float32[10,1,10]), ShapedArray(float32[1,1,10])).\n",
            "DEBUG:absl:Finished XLA compilation of add in 0.012429952621459961 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0002892017364501953 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337812096 for args (ShapedArray(float32[10,1,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of squeeze in 0.006795167922973633 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0002989768981933594 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337870800 for args ().\n",
            "DEBUG:absl:Finished XLA compilation of iota in 0.06382012367248535 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00039887428283691406 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338253728 for args (ShapedArray(int32[1,1]), ShapedArray(int32[])).\n",
            "DEBUG:absl:Finished XLA compilation of add in 0.06614136695861816 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0004208087921142578 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337849040 for args ().\n",
            "DEBUG:absl:Finished XLA compilation of iota in 0.00584721565246582 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00036716461181640625 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337868960 for args (ShapedArray(int32[1,1]), ShapedArray(int32[1,1])).\n",
            "DEBUG:absl:Finished XLA compilation of eq in 0.06510066986083984 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0003268718719482422 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338731520 for args (ShapedArray(bool[1,1]),).\n",
            "DEBUG:absl:Finished XLA compilation of convert_element_type in 0.06533074378967285 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0004100799560546875 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338190848 for args (ShapedArray(float32[1,1]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.003359079360961914 sec\n",
            "DEBUG:absl:Finished tracing + transforming _moveaxis for jit in 0.00043487548828125 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0003540515899658203 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337938592 for args (ShapedArray(float32[10,1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.01394033432006836 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0004093647003173828 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00039505958557128906 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337933760 for args (ShapedArray(bool[1,1]),).\n",
            "DEBUG:absl:Finished XLA compilation of reshape in 0.003178834915161133 sec\n",
            "DEBUG:absl:Finished tracing + transforming _where for jit in 0.0032062530517578125 sec\n",
            "DEBUG:absl:Compiling _where (140284337918912 for args (ShapedArray(bool[1,1,1,1]), ShapedArray(float32[10,1,1,2048,1]), ShapedArray(float32[])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(_where) in 0.01917719841003418 sec\n",
            "DEBUG:absl:Finished tracing + transforming _moveaxis for jit in 0.0003342628479003906 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0003635883331298828 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337956736 for args (ShapedArray(float32[10,1,1024]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.013467788696289062 sec\n",
            "DEBUG:absl:Finished tracing + transforming _where for jit in 0.002713441848754883 sec\n",
            "DEBUG:absl:Compiling _where (140284337931360 for args (ShapedArray(bool[1,1,1,1]), ShapedArray(float32[10,1,1,1024,1]), ShapedArray(float32[])).\n",
            "DEBUG:absl:Finished XLA compilation of vmap(_where) in 0.03285336494445801 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0003399848937988281 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337968544 for args (ShapedArray(float32[10,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.0031969547271728516 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00043964385986328125 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337926864 for args (ShapedArray(float32[10,1,10]), ShapedArray(float32[2048,10])).\n",
            "DEBUG:absl:Finished XLA compilation of dot_general in 0.016341447830200195 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005884170532226562 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284337978176 for args (ShapedArray(float32[10,10,1,2048]), ShapedArray(float32[2048,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of dot_general in 0.020853281021118164 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00045180320739746094 sec\n",
            "DEBUG:absl:Compiling prim_fun (140287053907520 for args (ShapedArray(float32[10,10,1,2048]),).\n",
            "DEBUG:absl:Finished XLA compilation of slice in 0.008401870727539062 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00041031837463378906 sec\n",
            "DEBUG:absl:Compiling prim_fun (140285108227760 for args (ShapedArray(float32[10,1,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of slice in 0.0032379627227783203 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.00037932395935058594 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338326256 for args (ShapedArray(float32[10,1,10]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.08675909042358398 sec\n",
            "DEBUG:absl:Finished tracing + transforming prim_fun for jit in 0.0005817413330078125 sec\n",
            "DEBUG:absl:Compiling prim_fun (140284338734416 for args (ShapedArray(float32[1,1,1]),).\n",
            "DEBUG:absl:Finished XLA compilation of broadcast_in_dim in 0.06552577018737793 sec\n",
            "DEBUG:absl:Finished tracing + transforming _moveaxis for jit in 0.0002810955047607422 sec\n",
            "DEBUG:absl:Finished tracing + transforming _einsum for jit in 0.001699209213256836 sec\n",
            "DEBUG:absl:Compiling _einsum (140284338206112 for args (ShapedArray(float32[10,10,1,2048]), ShapedArray(float32[10,1,1,1024,1]), ShapedArray(float32[10,1,1,1024,1]), ShapedArray(float32[10,10,1,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of _einsum in 0.05051422119140625 sec\n",
            "DEBUG:absl:Finished tracing + transforming fn for jit in 0.0006203651428222656 sec\n",
            "DEBUG:absl:Compiling fn (140284338075680 for args (ShapedArray(float32[]), ShapedArray(float32[10,10,10,10])).\n",
            "DEBUG:absl:Finished XLA compilation of fn in 0.021213293075561523 sec\n",
            "DEBUG:absl:Finished tracing + transforming _einsum for jit in 0.001966714859008789 sec\n",
            "DEBUG:absl:Compiling _einsum (140284338790880 for args (ShapedArray(float32[10,10,1,2048]), ShapedArray(float32[10,1,1,1]), ShapedArray(float32[10,1,1,1]), ShapedArray(float32[10,10,1,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of _einsum in 0.04493236541748047 sec\n",
            "DEBUG:absl:Finished tracing + transforming _einsum for jit in 0.0019483566284179688 sec\n",
            "DEBUG:absl:Compiling _einsum (140284337798672 for args (ShapedArray(float32[10,10,1,2048]), ShapedArray(float32[10,1,1,2048,1]), ShapedArray(float32[10,1,1,2048,1]), ShapedArray(float32[10,10,1,2048])).\n",
            "DEBUG:absl:Finished XLA compilation of _einsum in 0.04833364486694336 sec\n",
            "DEBUG:absl:Finished tracing + transforming _moveaxis for jit in 0.0002856254577636719 sec\n",
            "DEBUG:absl:Finished tracing + transforming _einsum for jit in 0.0016903877258300781 sec\n",
            "DEBUG:absl:Compiling _einsum (140284338213744 for args (ShapedArray(float32[10,10,1,10]), ShapedArray(float32[10,1,1,2048,1]), ShapedArray(float32[10,1,1,2048,1]), ShapedArray(float32[10,10,1,10])).\n",
            "DEBUG:absl:Finished XLA compilation of _einsum in 0.0461421012878418 sec\n",
            "DEBUG:absl:Finished tracing + transforming _einsum for jit in 0.001996278762817383 sec\n",
            "DEBUG:absl:Compiling _einsum (140284338298384 for args (ShapedArray(float32[10,10,1,10]), ShapedArray(float32[10,1,1,1]), ShapedArray(float32[10,1,1,1]), ShapedArray(float32[10,10,1,10])).\n",
            "DEBUG:absl:Finished XLA compilation of _einsum in 0.09563469886779785 sec\n",
            "DEBUG:absl:Finished tracing + transforming fn for jit in 0.0005786418914794922 sec\n",
            "DEBUG:absl:Compiling fn (140284338226352 for args (ShapedArray(float32[]), ShapedArray(float32[10,10,10,10])).\n",
            "DEBUG:absl:Finished XLA compilation of fn in 0.06949234008789062 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 10, 10, 10])\n"
          ]
        }
      ],
      "source": [
        "# Structured derivatives\n",
        "k_3 = ntk_fn_str_derivatives(x1, x2, params)\n",
        "print(k_3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Q63v1L1aggFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49c4375-a4ca-4a79-a570-be8d6dd100f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.0115e-06) tensor(3.0172e-06) tensor(2.0115e-06)\n"
          ]
        }
      ],
      "source": [
        "# Make sure kernels agree.\n",
        "print(\n",
        "    torch.max(torch.abs(k_1 - k_2)) / torch.mean(torch.abs(k_1)), \n",
        "    torch.max(torch.abs(k_1 - k_3)) / torch.mean(torch.abs(k_1)),\n",
        "    torch.max(torch.abs(k_2 - k_3)) / torch.mean(torch.abs(k_2))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ux7AEZ9fggFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08142415-00e7-4ff9-d8b2-425b5ff1827e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "impl=1, flops=2103388288.0\n",
            "impl=2, flops=1068667008.0\n",
            "impl=3, flops=2117817.0\n",
            "torch.Size([10, 10, 10, 10])\n"
          ]
        }
      ],
      "source": [
        "# Selects best method based on FLOPs at first call / compilation.\n",
        "# Takes about 3x more time to compile.\n",
        "# WARNING: due to an XLA issue, currently only works correctly on TPUs!\n",
        "# Wrong FLOPs for CPU/GPU of JITted functions.\n",
        "k_0 = ntk_fn_auto(x1, x2, params)\n",
        "print(k_0.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "diP7nkBuggFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff669f9-e6db-4ac3-c38e-8c6b88783ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 14.1 s per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "# Slow\n",
        "ntk_fn_jacobian_contraction(x1, x2, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wehCdvi2ggFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210248e0-4eed-441a-8153-7f8a9bd37382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 6.84 s per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "# 2X faster\n",
        "ntk_fn_ntvp(x1, x2, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Yrm53akVggFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47cd67a2-2b5a-48ea-a05e-15da2ef734a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 266 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "# 50X faster.\n",
        "ntk_fn_str_derivatives(x1, x2, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "P1QtkBqLggFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e1d404-c901-42a9-bef9-0deccbaeb1b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 265 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit \n",
        "# On TPU should match the fastest method.\n",
        "# On GPU/CPU, currently is broken, and may not be the fastest.\n",
        "ntk_fn_auto(x1, x2, params)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Sl2JyRE1hK-z",
        "oXqlToEouqSc"
      ],
      "name": "example_pytorch.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}